{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basil Dataset**\n",
    "\n",
    "Base code and description for this preprocessing was taken from the MTL project and only adjusted:\n",
    "Reshaped to a binary label: For now ONLY BIASED sentences in the dataset (coul eventually add all the other sentences as unbiased).\n",
    "\n",
    "\n",
    "A collection of 300 articles sampled between 2010 and 2019.\n",
    "For each article, the authors provide 2 files: An article file and a file containing annotations.\n",
    "Each article object contains the following keys: ['title', 'keywords', 'date', 'uuid', 'url', 'main-entities', 'word-count', 'source', 'main-event', 'triplet-uuid', 'body-paragraphs'] where the 'body-paragraph' contains the sentences of each paragraph.\n",
    "Each sentence-level-annotation objet contains the following keys: ['aim', 'bias', 'end', 'id', 'indirect-ally-opponent-sentiment', 'indirect-target-name', 'notes', 'polarity', 'quote', 'speaker', 'start', 'target', 'txt'].\n",
    "For our purposes, we extracted the labels bias, aim, quote, txt from each annotation.\n",
    "Additionally, we extracted the sentences contained in body-paragraphs from each article.\n",
    "We entirely discarded the article-level annotations.\n",
    "With these labels, we perform binary classification for three targets. Additionally, we can perform POS tagging for the bias-inducing POS (txt).\n",
    "Because of the quality of the dataset, we didn't have to discard any observation. The final dataset contains 1724 observations.\n",
    "\n",
    "[Dataset Source](https://github.com/launchnlp/BASIL)\n",
    "\n",
    "\n",
    "Domains of the columns:\n",
    "\n",
    "\n",
    "label:  Type of bias (inf=linguistic, lex=lexical)\n",
    "\n",
    "quote: Whether phrase is/ contains quote or not.\n",
    "\n",
    "aim: Whether a phrase is directly/ indirectly aiming at the target.\n",
    "\n",
    "pos: The sequence that induces the bias. This sequence is part of the sentence.\n",
    "\n",
    "text: The source sentence.\n",
    "\n",
    "\n",
    "Note: The authors used the term phrase instead of sentence. To ensure readability and comparability to other datasets, we used the term text.\n",
    "Citation Identifier: fan_plain_2019\n",
    "\n",
    "Title: In Plain Sight: Media Bias Through the Lens of Factual Reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from prep_collection import PrepCollection as prep\n",
    "#from preprocessors import PreprocessorBlueprint\n",
    "\n",
    "def _load_raw_data_from_local() -> Tuple[List, List]:\n",
    "    \"\"\"Load the raw data of 09_BASIL.\"\"\"\n",
    "    articles, annotations = [], []\n",
    "    for year in range(2010, 2020):\n",
    "        wdr_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "        ds_raw_path = os.path.join(wdr_path + \"/Datasets/Linguistic Bias/BASIL/\")\n",
    "        arts = sorted(os.listdir(os.path.join(ds_raw_path, \"articles\", str(year))))\n",
    "\n",
    "        anns = sorted(os.listdir(os.path.join(ds_raw_path, \"annotations\", str(year))))\n",
    "        anns_cut = [(\"\").join(ann.split(\"_ann\")) for ann in anns]\n",
    "\n",
    "        assert arts == anns_cut\n",
    "\n",
    "        for i, art in enumerate(arts):\n",
    "            try:\n",
    "                with open(os.path.join(ds_raw_path, \"articles\", str(year), art), \"r\", errors= 'replace') as f:\n",
    "                    article_data = json.load(f)\n",
    "\n",
    "                with open(os.path.join(ds_raw_path, \"annotations\", str(year), anns[i]), \"r\", errors= 'replace') as f:\n",
    "                    annotation_data = json.load(f)\n",
    "\n",
    "                articles.append(article_data)\n",
    "                annotations.append(annotation_data)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                print(\"Caught error. Attempted to load an empty file.\")\n",
    "\n",
    "    return articles, annotations\n",
    "\n",
    "\n",
    "def _preprocess():\n",
    "    \"\"\"Preprocess the raw data of 09_BASIL.\"\"\"\n",
    "    article_data, annotation_data = _load_raw_data_from_local()\n",
    "    observations = []\n",
    "    sent_id = 0\n",
    "    for i, art in enumerate(article_data):\n",
    "        ann = annotation_data[i]\n",
    "\n",
    "        paragraphs = art.get(\"body-paragraphs\")  # Now a list of paragraphs\n",
    "        annotations = ann.get(\"phrase-level-annotations\")  # Now a list of annotations\n",
    "\n",
    "        # For each paragraph, join the sentences together\n",
    "        phrases = list(itertools.chain.from_iterable(paragraphs))\n",
    "        for ann in annotations:\n",
    "            # The id can be in 2 different formats:\n",
    "            # \"p<prase-id>\" or \"title\"\n",
    "            id = ann[\"id\"]\n",
    "            text = ann[\"txt\"]\n",
    "            bias = ann[\"bias\"]  # Type of bias. (inf=linguistic, lex=lexical)\n",
    "            quote = ann[\"quote\"]  # Binary, whether phrase is/ contains quote or not.\n",
    "            aim = ann[\"aim\"]  # direct/ indirect. If indirect, annotations for ...-sentiment are availabe.\n",
    "            if id == \"title\":\n",
    "                phrase = art.get(\"title\")\n",
    "            else:\n",
    "                id = int(id.split(\"p\")[-1])\n",
    "                phrase = phrases[id]\n",
    "\n",
    "            observation = {\"id\": sent_id, \"text\": prep.prepare_text(phrase), \"label\": 1, \"pos\": text, \"bias_type\": bias, \"quote\": quote, \"aim\": aim}\n",
    "            observations.append(observation)\n",
    "            sent_id += 1\n",
    "\n",
    "    df = pd.DataFrame(observations)\n",
    "    df.to_csv(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())) + \"/Preprocessed_Datasets/009-Basil.csv\"))\n",
    "\n",
    "\n",
    "_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
