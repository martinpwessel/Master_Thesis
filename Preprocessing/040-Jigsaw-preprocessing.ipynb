{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaggle Jigsaw Dataset**\n",
    "\n",
    "Datasource: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv\n",
    "\n",
    "\"This attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic).[...]The competition target was a binarized version of the toxicity column, which can be easily reconstructed using a >=0.5 threshold.\" -  That is what I replicated for the binary label\n",
    "\n",
    "Toxicity is defined in the paper as: \"toxicity is defined as anything rude, disrespectful, or otherwise likely to make someone leave a discussion\"\n",
    "\n",
    "```\n",
    "Other categories include\n",
    "    severe_toxicity\n",
    "    obscene\n",
    "    threat\n",
    "    insult\n",
    "    identity_attack\n",
    "    sexual_explicit\n",
    "```\n",
    "\n",
    "```\n",
    "LABEL:\n",
    "0 - NEUTRAL\n",
    "1 - TOXIC\n",
    "```\n",
    "\n",
    "Also `LABEL_FLOAT` given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from prep_collection import PrepCollection as prep\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wdr_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "ds_raw_path = os.path.join(wdr_path + \"/Datasets/Hate Speech/Jigsaw Dataset\")\n",
    "file = os.listdir(ds_raw_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_jigsaw(wdr_path, ds_raw_path, file):\n",
    "    df_original = pd.read_csv(os.path.join(ds_raw_path, file))\n",
    "    df = pd.DataFrame()\n",
    "    df['text'] = df_original['comment_text'].apply(lambda x: prep.prepare_text(str(x)))\n",
    "    df['label'] = df_original['toxicity']\n",
    "    df['label_float']  = df_original['toxicity']\n",
    "    df.loc[df['label'] >= 0.5, 'label'] = 1\n",
    "    df.loc[df['label'] < 0.5, 'label'] = 0\n",
    "    df['id'] = df.index\n",
    "    df.to_csv(os.path.join(wdr_path + \"/Preprocessed_Datasets/040-Jigsaw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_jigsaw(wdr_path, ds_raw_path, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
